# Soluciones Tutorial 2: Homolog√≠a Persistente Avanzada

Este documento contiene las soluciones completas y explicaciones detalladas para todos los ejercicios del Tutorial 2 v2.

---

## Ejercicio 1: generate_brain_state_realistic

### Soluci√≥n Completa:

```python
def generate_brain_state_realistic(state_type, n_neurons=100, noise=0.1):
    """
    Genera estados cerebrales sint√©ticos con propiedades realistas.
    """
    if state_type == 'sleep':
        # Sue√±o: activaci√≥n sincronizada, baja dimensionalidad
        base = np.random.randn(n_neurons, 1) @ np.random.randn(1, 5)
        data = base + np.random.randn(n_neurons, 5) * noise

    elif state_type == 'wakeful':
        # Vigilia: activaci√≥n dispersa, alta dimensionalidad
        data = np.random.randn(n_neurons, 5) * 1.5

    elif state_type == 'attention':
        # Atenci√≥n: subredes focales activas
        data = np.zeros((n_neurons, 5))
        data[:n_neurons//3] = np.random.randn(n_neurons//3, 5) * 2.0
        data[n_neurons//3:] = np.random.randn(2*n_neurons//3, 5) * 0.3

    elif state_type == 'memory':
        # Memoria: patrones c√≠clicos (bucles de retroalimentaci√≥n)
        theta = np.linspace(0, 4*np.pi, n_neurons)
        data = np.column_stack([
            np.cos(theta),
            np.sin(theta),
            np.cos(2*theta) * 0.5,
            np.sin(2*theta) * 0.5,
            np.random.randn(n_neurons) * noise
        ])

    return data
```

### Explicaci√≥n Paso a Paso:

#### **Sleep (Sue√±o):**
- **Objetivo:** Modelar activaci√≥n sincronizada de baja dimensionalidad
- **M√©todo:** Proyecci√≥n desde 1D a 5D
  1. `np.random.randn(n_neurons, 1)` crea un vector columna (factor com√∫n)
  2. `@ np.random.randn(1, 5)` proyecta a 5 dimensiones
  3. El resultado: todas las neuronas var√≠an juntas (correlacionadas)
  4. `+ noise` agrega variaci√≥n individual peque√±a

**Intuici√≥n neurobiol√≥gica:** Durante el sue√±o, las neuronas tienden a activarse de forma sincronizada (ondas lentas), reduciendo la dimensionalidad efectiva del espacio de estados.

#### **Wakeful (Vigilia):**
- **Objetivo:** Activaci√≥n dispersa e independiente
- **M√©todo:** Simple ruido gaussiano
  - `np.random.randn(n_neurons, 5) * 1.5` genera puntos aleatorios en 5D
  - Cada neurona es independiente
  - Factor 1.5 aumenta la dispersi√≥n

**Intuici√≥n neurobiol√≥gica:** Durante vigilia activa, las neuronas tienen patrones de activaci√≥n m√°s diversos y menos correlacionados.

#### **Attention (Atenci√≥n):**
- **Objetivo:** Subred focal altamente activa
- **M√©todo:** Activaci√≥n diferencial
  1. `data = np.zeros((n_neurons, 5))` inicializa todo en 0
  2. Primera tercera parte: alta actividad (`* 2.0`)
  3. Resto: actividad basal baja (`* 0.3`)

**Intuici√≥n neurobiol√≥gica:** La atenci√≥n selectiva implica que una subred espec√≠fica (ej., corteza prefrontal) est√° muy activa mientras otras regiones tienen actividad basal.

#### **Memory (Memoria):**
- **Objetivo:** Estructura c√≠clica (bucles de retroalimentaci√≥n)
- **M√©todo:** Funciones peri√≥dicas
  1. `theta = np.linspace(0, 4*np.pi, n_neurons)` crea √°ngulos
  2. `cos(theta), sin(theta)` forman un bucle principal
  3. `cos(2*theta), sin(2*theta)` agregan un segundo arm√≥nico
  4. Quinta dimensi√≥n: ruido

**Intuici√≥n neurobiol√≥gica:** Los bucles de retroalimentaci√≥n en redes neuronales (como en memoria de trabajo) crean trayectorias c√≠clicas en el espacio de estados.

### Consejos de Debugging:

**Error com√∫n 1:** `ValueError: operands could not be broadcast`
- **Causa:** Dimensiones incompatibles en multiplicaci√≥n de matrices
- **Soluci√≥n:** Verifica que `(n_neurons, 1) @ (1, 5)` produce `(n_neurons, 5)`

**Error com√∫n 2:** `TypeError: unsupported operand type(s) for @`
- **Causa:** Usando `*` en lugar de `@` para multiplicaci√≥n matricial
- **Soluci√≥n:** Usa `@` para multiplicaci√≥n de matrices, no `*`

**Error com√∫n 3:** Estado 'memory' no tiene ciclos
- **Causa:** `theta` no tiene rango suficiente para completar ciclos
- **Soluci√≥n:** Usa `4*np.pi` o m√°s para hacer al menos 2 ciclos completos

---

## Ejercicio 2: generate_spike_trains

### Soluci√≥n Completa:

```python
def generate_spike_trains(n_neurons=20, duration=1000, base_rate=5.0,
                         correlation=0.3, pattern_type='random'):
    """
    Genera spike trains sint√©ticos con diferentes patrones.
    """
    spike_trains = np.zeros((n_neurons, duration))

    if pattern_type == 'random':
        # Actividad aleatoria independiente
        for i in range(n_neurons):
            spike_trains[i] = poisson.rvs(base_rate/1000, size=duration)

    elif pattern_type == 'synchronized':
        # Actividad sincronizada
        common_pattern = poisson.rvs(base_rate/1000, size=duration)
        for i in range(n_neurons):
            spike_trains[i] = common_pattern * (np.random.rand(duration) < 0.8)

    elif pattern_type == 'sequential':
        # Actividad secuencial
        for t in range(duration):
            active_neuron = (t // 20) % n_neurons
            spike_trains[active_neuron, t] = poisson.rvs(base_rate*3/1000)

    return spike_trains
```

### Explicaci√≥n Paso a Paso:

#### **Random (Aleatorio):**
- **Distribuci√≥n de Poisson:** Modelo est√°ndar para spikes neuronales
- `base_rate/1000` convierte Hz a probabilidad por ms
- `size=duration` genera un spike train completo
- Cada neurona es **independiente**

**Matem√°tica:**
$$P(\\text{spike en t}) = \\lambda \\Delta t$$
donde $\\lambda$ = `base_rate` (Hz), $\\Delta t$ = 1 ms

#### **Synchronized (Sincronizado):**
- `common_pattern`: Patr√≥n maestro compartido por todas
- `* (np.random.rand(duration) < 0.8)`: Cada neurona sigue el patr√≥n con 80% de probabilidad
- Resultado: Alta correlaci√≥n entre neuronas

**¬øPor qu√© 80%?** Para simular sincronizaci√≥n imperfecta (realismo biol√≥gico). Sincronizaci√≥n perfecta (100%) es rara en el cerebro real.

#### **Sequential (Secuencial):**
- `(t // 20) % n_neurons`: Elige qu√© neurona activa en cada tiempo
  - `t // 20`: Cambia cada 20 ms
  - `% n_neurons`: Cicla entre todas las neuronas
- `base_rate*3`: La neurona activa dispara m√°s frecuentemente
- Resultado: Onda de activaci√≥n que recorre la poblaci√≥n

**Visualizaci√≥n:**
```
t=0-19:   Neurona 0 activa  ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
t=20-39:  Neurona 1 activa  ‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë
t=40-59:  Neurona 2 activa  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñë‚ñë
...y as√≠ sucesivamente
```

### Consejos de Debugging:

**Error com√∫n 1:** `AttributeError: module 'scipy.stats' has no attribute 'poisson'`
- **Causa:** No importaste `from scipy.stats import poisson`
- **Soluci√≥n:** Aseg√∫rate de tener la importaci√≥n correcta

**Error com√∫n 2:** Spikes demasiado frecuentes (matriz llena de 1s)
- **Causa:** No dividiste `base_rate` por 1000
- **Soluci√≥n:** `base_rate/1000` para convertir Hz ‚Üí probabilidad/ms

**Error com√∫n 3:** Patr√≥n secuencial no es visible
- **Causa:** Cambio muy r√°pido (`t // 5`) o muy lento (`t // 100`)
- **Soluci√≥n:** Usa `t // 20` para ~20 ms por neurona

---

## Ejercicio 3: spike_trains_to_state_space

### Soluci√≥n Completa:

```python
def spike_trains_to_state_space(spike_trains, bin_size=50, stride=25):
    """
    Convierte spike trains a representaci√≥n en espacio de estados.
    """
    n_neurons, duration = spike_trains.shape
    n_bins = (duration - bin_size) // stride + 1

    state_space = np.zeros((n_bins, n_neurons))

    for i in range(n_bins):
        start = i * stride
        end = start + bin_size
        state_space[i] = np.sum(spike_trains[:, start:end], axis=1)

    return state_space
```

### Explicaci√≥n Paso a Paso:

#### **C√°lculo del n√∫mero de bins:**
```python
n_bins = (duration - bin_size) // stride + 1
```
- Ejemplo: `duration=1000`, `bin_size=50`, `stride=25`
- `(1000 - 50) // 25 + 1 = 950 // 25 + 1 = 38 + 1 = 39 bins`

**¬øPor qu√© esta f√≥rmula?**
- Primera ventana: `[0, 50)`
- Segunda ventana: `[25, 75)` (overlap de 25 ms)
- √öltima ventana: `[950, 1000)`
- Total: 39 ventanas

#### **Ventana deslizante:**
```python
for i in range(n_bins):
    start = i * stride          # 0, 25, 50, 75, ...
    end = start + bin_size      # 50, 75, 100, 125, ...
```

#### **Conteo de spikes:**
```python
state_space[i] = np.sum(spike_trains[:, start:end], axis=1)
```
- `spike_trains[:, start:end]`: Todas las neuronas, ventana temporal
- `axis=1`: Suma a lo largo del tiempo
- Resultado: Vector de conteos (un valor por neurona)

**Visualizaci√≥n:**
```
spike_trains:
Neurona 0: |-----|---------|---|-----|--------|
Neurona 1: |---|-----|----------|--|----------|
           [   Bin 1  ][  Bin 2   ][  Bin 3  ]

state_space:
Bin 1 ‚Üí [3, 2]  # Neurona 0: 3 spikes, Neurona 1: 2 spikes
Bin 2 ‚Üí [1, 3]
Bin 3 ‚Üí [2, 2]
```

### Consejos de Debugging:

**Error com√∫n 1:** N√∫mero incorrecto de bins
- **Causa:** F√≥rmula incorrecta `duration // stride`
- **Soluci√≥n:** Usa `(duration - bin_size) // stride + 1`

**Error com√∫n 2:** `IndexError: index out of bounds`
- **Causa:** √öltima ventana excede `duration`
- **Soluci√≥n:** Aseg√∫rate de que `end = start + bin_size` est√© dentro de l√≠mites

**Error com√∫n 3:** Valores muy altos en `state_space`
- **Causa:** Sumaste sobre el eje incorrecto
- **Soluci√≥n:** Usa `axis=1` para sumar a lo largo del tiempo, no neuronas

---

## Ejercicio 4: extract_topological_features

### Soluci√≥n Completa:

```python
def extract_topological_features(diagram, dim=1):
    """
    Extrae caracter√≠sticas escalares de un diagrama de persistencia.
    """
    features = {}

    if len(diagram[dim]) == 0:
        return {'n_features': 0, 'max_persistence': 0,
                'mean_persistence': 0, 'std_persistence': 0,
                'total_persistence': 0, 'entropy': 0}

    # Filtrar puntos infinitos
    dgm = diagram[dim][np.isfinite(diagram[dim][:, 1])]

    if len(dgm) == 0:
        return {'n_features': 0, 'max_persistence': 0,
                'mean_persistence': 0, 'std_persistence': 0,
                'total_persistence': 0, 'entropy': 0}

    # Calcular lifetimes
    lifetimes = dgm[:, 1] - dgm[:, 0]

    # Caracter√≠sticas b√°sicas
    features['n_features'] = len(dgm)
    features['max_persistence'] = np.max(lifetimes)
    features['mean_persistence'] = np.mean(lifetimes)
    features['std_persistence'] = np.std(lifetimes)
    features['total_persistence'] = np.sum(lifetimes)

    # Entrop√≠a de persistencia
    if np.sum(lifetimes) > 0:
        probs = lifetimes / np.sum(lifetimes)
        entropy = -np.sum(probs * np.log(probs + 1e-10))
        features['entropy'] = entropy
    else:
        features['entropy'] = 0

    return features
```

### Explicaci√≥n Paso a Paso:

#### **1. Manejo de diagramas vac√≠os:**
```python
if len(diagram[dim]) == 0:
    return {all zeros}
```
- **Crucial:** Sin esto, c√≥digo falla con `IndexError`
- Retorna caracter√≠sticas nulas para indicar "sin topolog√≠a"

#### **2. Filtrar puntos infinitos:**
```python
dgm = diagram[dim][np.isfinite(diagram[dim][:, 1])]
```
- **¬øPor qu√©?** Puntos con `death = ‚àû` representan caracter√≠sticas que nunca mueren
- Estas son usualmente caracter√≠sticas H‚ÇÄ que persisten siempre
- No podemos calcular `lifetime = ‚àû - birth`
- `np.isfinite(...)` retorna `True` solo para valores finitos

#### **3. Calcular lifetimes (persistencias):**
```python
lifetimes = dgm[:, 1] - dgm[:, 0]
```
- `dgm[:, 0]`: Columna de births
- `dgm[:, 1]`: Columna de deaths
- **Interpretaci√≥n:** ¬øCu√°nto "vivi√≥" cada caracter√≠stica topol√≥gica?

**Ejemplo:**
```
Point: (birth=0.2, death=0.8) ‚Üí lifetime = 0.6 (muy persistente)
Point: (birth=0.5, death=0.51) ‚Üí lifetime = 0.01 (ruido)
```

#### **4. Estad√≠sticas b√°sicas:**
- `max_persistence`: Caracter√≠stica m√°s robusta
- `mean_persistence`: Persistencia t√≠pica
- `std_persistence`: Variabilidad
- `total_persistence`: "Cantidad total" de topolog√≠a

#### **5. Entrop√≠a de persistencia:**
```python
probs = lifetimes / np.sum(lifetimes)
entropy = -np.sum(probs * np.log(probs + 1e-10))
```

**Interpretaci√≥n:**
- Normaliza lifetimes a distribuci√≥n de probabilidad
- Calcula entrop√≠a de Shannon: $H = -\\sum p_i \\log(p_i)$
- **Entrop√≠a alta:** Muchas caracter√≠sticas de persistencia similar
- **Entrop√≠a baja:** Una o pocas caracter√≠sticas dominantes

**Ejemplo:**
```
Caso 1: lifetimes = [0.1, 0.1, 0.1, 0.1, 0.1]
‚Üí probs = [0.2, 0.2, 0.2, 0.2, 0.2]
‚Üí entropy ‚âà 1.6 (ALTA - uniforme)

Caso 2: lifetimes = [0.9, 0.01, 0.01, 0.01, 0.07]
‚Üí probs ‚âà [0.9, 0.01, 0.01, 0.01, 0.07]
‚Üí entropy ‚âà 0.5 (BAJA - dominada por una)
```

**¬øPor qu√© +1e-10?** Evitar `log(0) = -‚àû` en caso de probabilidades exactamente 0.

### Consejos de Debugging:

**Error com√∫n 1:** `RuntimeWarning: divide by zero in log`
- **Causa:** No agregaste `+ 1e-10` en el logaritmo
- **Soluci√≥n:** Siempre usa `np.log(probs + 1e-10)`

**Error com√∫n 2:** `IndexError: index 1 is out of bounds`
- **Causa:** Accediste `diagram[dim]` pero `dim` est√° fuera de rango
- **Soluci√≥n:** Verifica que el diagrama tenga al menos `dim+1` dimensiones

**Error com√∫n 3:** Entrop√≠a negativa
- **Causa:** Error en la f√≥rmula (signo)
- **Soluci√≥n:** Debe ser `-np.sum(...)` (negativo)

---

## üéì Ejercicios Adicionales (Desaf√≠os)

### Desaf√≠o 1: Distancias entre Estados
Calcula la matriz de distancias Bottleneck entre todos los pares de estados cerebrales.

**Pista:**
```python
from persim import bottleneck

for i in range(len(states)):
    for j in range(i+1, len(states)):
        dist = bottleneck(diagrams[i][1], diagrams[j][1])
```

### Desaf√≠o 2: Clasificador Topol√≥gico
Usa caracter√≠sticas topol√≥gicas para entrenar un Random Forest que clasifique patrones de spike trains.

**Pista:**
```python
from sklearn.ensemble import RandomForestClassifier

# Generar m√∫ltiples ejemplos
X = []  # caracter√≠sticas topol√≥gicas
y = []  # etiquetas de patr√≥n

# Entrenar
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
```

### Desaf√≠o 3: Evoluci√≥n Temporal
Estudia c√≥mo cambia la topolog√≠a durante una transici√≥n de estados.

**Pista:**
```python
# Crear transici√≥n gradual
alpha = np.linspace(0, 1, 50)
for a in alpha:
    mixed = (1-a)*state1 + a*state2
    # Calcular topolog√≠a
```

---

## üìö Recursos Adicionales

### Papers Recomendados:
1. Giusti et al. (2015). "Clique topology reveals intrinsic structure in neural correlations"
2. Petri et al. (2014). "Homological scaffolds of brain functional networks"
3. Sizemore et al. (2019). "Cliques and cavities in the human connectome"

### Documentaci√≥n:
- [Ripser](https://ripser.scikit-tda.org/): Homolog√≠a persistente r√°pida
- [Persim](https://persim.scikit-tda.org/): Distancias entre diagramas
- [Giotto-TDA](https://giotto-ai.github.io/gtda-docs/): Suite completa

---

## ü§ù ¬øPreguntas?

Si tienes dudas sobre las soluciones:
1. Revisa los comentarios en el c√≥digo
2. Compara con los tests autom√°ticos
3. Consulta la documentaci√≥n de las bibliotecas
4. Abre un issue en el repositorio

**¬°Buen trabajo completando el Tutorial 2!** üéâ

---

**√öltima actualizaci√≥n:** 2025-01-15
**Autor:** MARK-126
