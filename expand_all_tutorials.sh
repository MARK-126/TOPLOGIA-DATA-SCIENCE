#!/bin/bash

echo "ğŸš€ Expandiendo todos los tutoriales con ejercicios adicionales..."
echo ""

cd /home/user/TOPLOGIA-DATA-SCIENCE

# Ya expandimos Tutorial 1
echo "âœ… Tutorial 1: Ya expandido (7 ejercicios)"

# Crear scripts para los demÃ¡s
echo "ğŸ“ Creando scripts de expansiÃ³n..."

# Los scripts se crearÃ¡n en Python
python3 << 'EOF'
import nbformat as nbf

# =========================================================================
# TUTORIAL 2: Agregar 3 ejercicios nuevos
# =========================================================================
print("\nğŸ”§ Expandiendo Tutorial 2...")

with open('notebooks/02_Homologia_Persistente_Avanzada_v2.ipynb', 'r') as f:
    nb2 = nbf.read(f, as_version=4)

# Ejercicios nuevos para Tutorial 2:
# 5. compute_wasserstein_distance - Distancia de Wasserstein entre diagramas
# 6. detect_temporal_changes - Detectar cambios en series temporales
# 7. classify_spike_patterns - Clasificar patrones de spikes con TDA

insert_idx2 = len(nb2.cells) - 1

new_cells2 = []

# Ejercicio 5
new_cells2.append(nbf.v4.new_markdown_cell("""### Ejercicio 5 - compute_wasserstein_distance

Calcula la distancia de Wasserstein entre dos diagramas de persistencia.

**Objetivo:** Cuantificar similitud entre patrones de spikes

**Concepto:** La distancia de Wasserstein mide el "costo" de transformar un diagrama en otro."""))

new_cells2.append(nbf.v4.new_code_cell("""# EJERCICIO 5: Distancia de Wasserstein

def compute_wasserstein_distance(dgm1, dgm2, order=2):
    \"\"\"
    Calcula distancia de Wasserstein entre dos diagramas.

    Arguments:
    dgm1, dgm2 -- diagramas de persistencia
    order -- orden de la distancia (1 o 2)

    Returns:
    distance -- distancia de Wasserstein
    \"\"\"
    from persim import sliced_wasserstein

    # 1. Filtrar puntos infinitos
    # (approx. 4 lines)
    # YOUR CODE STARTS HERE




    # YOUR CODE ENDS HERE

    # 2. Calcular distancia usando persim
    # (approx. 2 lines)
    # YOUR CODE STARTS HERE


    # YOUR CODE ENDS HERE

    return distance"""))

# Ejercicio 6  
new_cells2.append(nbf.v4.new_markdown_cell("""### Ejercicio 6 - detect_temporal_changes

Detecta cambios temporales en actividad neuronal usando persistencia deslizante.

**AplicaciÃ³n:** DetecciÃ³n de crisis epilÃ©pticas, transiciones de estados"""))

new_cells2.append(nbf.v4.new_code_cell("""# EJERCICIO 6: DetecciÃ³n de Cambios Temporales

def detect_temporal_changes(spike_train, window_size=100, stride=50):
    \"\"\"
    Detecta cambios usando ventanas deslizantes de persistencia.

    Arguments:
    spike_train -- tiempos de spikes (array 1D)
    window_size -- tamaÃ±o de ventana
    stride -- paso entre ventanas

    Returns:
    time_points -- puntos temporales centrales
    persistence_values -- persistencia mÃ¡xima en cada ventana
    change_points -- Ã­ndices donde hay cambios significativos
    \"\"\"

    # 1. Ventanas deslizantes
    # (approx. 6 lines)
    # YOUR CODE STARTS HERE






    # YOUR CODE ENDS HERE

    # 2. Detectar cambios (gradiente alto)
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    return time_points, persistence_values, change_points"""))

# Ejercicio 7
new_cells2.append(nbf.v4.new_markdown_cell("""### Ejercicio 7 - classify_spike_patterns

Clasifica patrones de spikes usando caracterÃ­sticas topolÃ³gicas.

**Objetivo:** Distinguir entre tipos de actividad neuronal"""))

new_cells2.append(nbf.v4.new_code_cell("""# EJERCICIO 7: ClasificaciÃ³n de Patrones

def classify_spike_patterns(spike_trains_dict, test_size=0.3):
    \"\"\"
    Clasifica patrones usando Random Forest y features TDA.

    Arguments:
    spike_trains_dict -- dict {pattern_name: [spike_trains_list]}
    test_size -- proporciÃ³n de test

    Returns:
    clf -- clasificador entrenado
    accuracy -- precisiÃ³n en test
    \"\"\"
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split

    X_data = []
    y_data = []

    # 1. Extraer features TDA de cada spike train
    # (approx. 10 lines)
    # YOUR CODE STARTS HERE










    # YOUR CODE ENDS HERE

    # 2. Train/test split y clasificaciÃ³n
    # (approx. 5 lines)
    # YOUR CODE STARTS HERE





    # YOUR CODE ENDS HERE

    return clf, accuracy"""))

# Insertar nuevas celdas
header2 = nbf.v4.new_markdown_cell("""## Ejercicios Avanzados: AnÃ¡lisis Comparativo

---""")
nb2.cells.insert(insert_idx2, header2)
for i, cell in enumerate(new_cells2):
    nb2.cells.insert(insert_idx2 + 1 + i, cell)

with open('notebooks/02_Homologia_Persistente_Avanzada_v2.ipynb', 'w') as f:
    nbf.write(nb2, f)

print("âœ… Tutorial 2 expandido: 7 ejercicios totales (agregados 3 nuevos)")

# =========================================================================
# TUTORIAL 3: Conectividad Cerebral - Agregar 3 ejercicios
# =========================================================================
print("\nğŸ”§ Expandiendo Tutorial 3...")

with open('notebooks/03_Conectividad_Cerebral_v2.ipynb', 'r') as f:
    nb3 = nbf.read(f, as_version=4)

insert_idx3 = len(nb3.cells) - 1
new_cells3 = []

# Ejercicio 4
new_cells3.append(nbf.v4.new_markdown_cell("""### Ejercicio 4 - compute_graph_features

Calcula caracterÃ­sticas de grafo complementarias a TDA.

**Incluir:** Clustering coefficient, betweenness centrality, modularity"""))

new_cells3.append(nbf.v4.new_code_cell("""# EJERCICIO 4: CaracterÃ­sticas de Grafo

def compute_graph_features(conn_matrix, threshold=0.3):
    \"\"\"
    Calcula caracterÃ­sticas de teorÃ­a de grafos.

    Arguments:
    conn_matrix -- matriz de conectividad
    threshold -- umbral para binarizar

    Returns:
    features -- dict con caracterÃ­sticas del grafo
    \"\"\"
    import networkx as nx

    # 1. Crear grafo desde matriz de conectividad
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    # 2. Calcular mÃ©tricas
    # (approx. 8 lines)
    # clustering, betweenness, degree distribution, etc.
    # YOUR CODE STARTS HERE








    # YOUR CODE ENDS HERE

    return features"""))

# Ejercicio 5
new_cells3.append(nbf.v4.new_markdown_cell("""### Ejercicio 5 - find_critical_nodes

Identifica nodos crÃ­ticos usando persistencia local.

**AplicaciÃ³n:** Encontrar regiones cerebrales esenciales"""))

new_cells3.append(nbf.v4.new_code_cell("""# EJERCICIO 5: Nodos CrÃ­ticos

def find_critical_nodes(conn_matrix, top_k=5):
    \"\"\"
    Identifica nodos mÃ¡s crÃ­ticos topolÃ³gicamente.

    Arguments:
    conn_matrix -- matriz de conectividad
    top_k -- nÃºmero de nodos crÃ­ticos a retornar

    Returns:
    critical_nodes -- Ã­ndices de nodos crÃ­ticos
    criticality_scores -- scores de criticidad
    \"\"\"

    # 1. Para cada nodo, calcular su impacto en conectividad global
    # (approx. 10 lines)
    # Remover temporalmente cada nodo y medir cambio en Î²â‚€
    # YOUR CODE STARTS HERE










    # YOUR CODE ENDS HERE

    # 2. Ordenar y retornar top k
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    return critical_nodes, criticality_scores"""))

# Ejercicio 6
new_cells3.append(nbf.v4.new_markdown_cell("""### Ejercicio 6 - track_connectivity_evolution

Analiza cÃ³mo evoluciona la conectividad en el tiempo.

**AplicaciÃ³n:** Plasticidad sinÃ¡ptica, aprendizaje"""))

new_cells3.append(nbf.v4.new_code_cell("""# EJERCICIO 6: EvoluciÃ³n de Conectividad

def track_connectivity_evolution(connectivity_matrices, time_points):
    \"\"\"
    Rastrea evoluciÃ³n temporal de caracterÃ­sticas topolÃ³gicas.

    Arguments:
    connectivity_matrices -- lista de matrices en diferentes tiempos
    time_points -- puntos temporales correspondientes

    Returns:
    evolution_features -- dict con series temporales de features
    \"\"\"

    # 1. Para cada matriz, extraer features topolÃ³gicas
    # (approx. 12 lines)
    # YOUR CODE STARTS HERE












    # YOUR CODE ENDS HERE

    return evolution_features"""))

header3 = nbf.v4.new_markdown_cell("""## Ejercicios Avanzados: AnÃ¡lisis de Redes

---""")
nb3.cells.insert(insert_idx3, header3)
for i, cell in enumerate(new_cells3):
    nb3.cells.insert(insert_idx3 + 1 + i, cell)

with open('notebooks/03_Conectividad_Cerebral_v2.ipynb', 'w') as f:
    nbf.write(nb3, f)

print("âœ… Tutorial 3 expandido: 6 ejercicios totales (agregados 3 nuevos)")

# =========================================================================
# TUTORIAL 4: Mapper - Agregar 2 ejercicios
# =========================================================================
print("\nğŸ”§ Expandiendo Tutorial 4...")

with open('notebooks/04_Mapper_Algorithm_v2.ipynb', 'r') as f:
    nb4 = nbf.read(f, as_version=4)

insert_idx4 = len(nb4.cells) - 1
new_cells4 = []

# Ejercicio 4
new_cells4.append(nbf.v4.new_markdown_cell("""### Ejercicio 4 - optimize_mapper_parameters

Optimiza parÃ¡metros de Mapper para mejor visualizaciÃ³n.

**Objetivo:** Encontrar n_intervals y overlap Ã³ptimos"""))

new_cells4.append(nbf.v4.new_code_cell("""# EJERCICIO 4: OptimizaciÃ³n de ParÃ¡metros

def optimize_mapper_parameters(data, filter_values, param_grid):
    \"\"\"
    Busca mejores parÃ¡metros de Mapper.

    Arguments:
    data -- datos originales
    filter_values -- valores del filtro
    param_grid -- dict con rangos {'n_intervals': [...], 'overlap': [...]}

    Returns:
    best_params -- mejores parÃ¡metros encontrados
    best_score -- score de calidad
    \"\"\"

    # 1. Grid search sobre parÃ¡metros
    # (approx. 12 lines)
    # MÃ©trica: maximizar nÃºmero de nodos sin fragmentar
    # YOUR CODE STARTS HERE












    # YOUR CODE ENDS HERE

    return best_params, best_score"""))

# Ejercicio 5  
new_cells4.append(nbf.v4.new_markdown_cell("""### Ejercicio 5 - detect_loops_in_mapper

Detecta loops (ciclos) en el grafo de Mapper.

**AplicaciÃ³n:** Identificar procesos recurrentes en dinÃ¡micas cerebrales"""))

new_cells4.append(nbf.v4.new_code_cell("""# EJERCICIO 5: DetecciÃ³n de Loops

def detect_loops_in_mapper(G):
    \"\"\"
    Detecta ciclos en el grafo de Mapper.

    Arguments:
    G -- grafo de NetworkX

    Returns:
    loops -- lista de ciclos encontrados
    loop_lengths -- longitudes de cada ciclo
    \"\"\"
    import networkx as nx

    # 1. Encontrar todos los ciclos simples
    # (approx. 4 lines)
    # YOUR CODE STARTS HERE




    # YOUR CODE ENDS HERE

    # 2. Clasificar por longitud
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    return loops, loop_lengths"""))

header4 = nbf.v4.new_markdown_cell("""## Ejercicios Avanzados: OptimizaciÃ³n

---""")
nb4.cells.insert(insert_idx4, header4)
for i, cell in enumerate(new_cells4):
    nb4.cells.insert(insert_idx4 + 1 + i, cell)

with open('notebooks/04_Mapper_Algorithm_v2.ipynb', 'w') as f:
    nbf.write(nb4, f)

print("âœ… Tutorial 4 expandido: 5 ejercicios totales (agregados 2 nuevos)")

# =========================================================================
# TUTORIAL 5: Series Temporales - Agregar 3 ejercicios
# =========================================================================
print("\nğŸ”§ Expandiendo Tutorial 5...")

with open('notebooks/05_Series_Temporales_EEG_v2.ipynb', 'r') as f:
    nb5 = nbf.read(f, as_version=4)

insert_idx5 = len(nb5.cells) - 1
new_cells5 = []

# Ejercicio 4
new_cells5.append(nbf.v4.new_markdown_cell("""### Ejercicio 4 - compute_delay_embedding_dim

Estima dimensiÃ³n Ã³ptima de embedding usando False Nearest Neighbors.

**Objetivo:** Determinar dim Ã³ptima para reconstrucciÃ³n"""))

new_cells5.append(nbf.v4.new_code_cell("""# EJERCICIO 4: DimensiÃ³n de Embedding Ã“ptima

def compute_delay_embedding_dim(signal, delay, max_dim=10):
    \"\"\"
    Estima dimensiÃ³n Ã³ptima de embedding.

    Arguments:
    signal -- seÃ±al temporal 1D
    delay -- delay a usar
    max_dim -- dimensiÃ³n mÃ¡xima a probar

    Returns:
    optimal_dim -- dimensiÃ³n Ã³ptima estimada
    fnn_percentages -- porcentajes de FNN para cada dim
    \"\"\"

    # 1. MÃ©todo de False Nearest Neighbors
    # (approx. 15 lines)
    # Para cada dim, contar % de vecinos "falsos"
    # YOUR CODE STARTS HERE















    # YOUR CODE ENDS HERE

    return optimal_dim, fnn_percentages"""))

# Ejercicio 5
new_cells5.append(nbf.v4.new_markdown_cell("""### Ejercicio 5 - reconstruct_attractor

Reconstruye el atractor desde una serie temporal.

**AplicaciÃ³n:** AnÃ¡lisis de dinÃ¡micas cerebrales complejas"""))

new_cells5.append(nbf.v4.new_code_cell("""# EJERCICIO 5: ReconstrucciÃ³n de Atractor

def reconstruct_attractor(signal, delay=None, dimension=3):
    \"\"\"
    Reconstruye atractor con Takens + analiza topologÃ­a.

    Arguments:
    signal -- seÃ±al temporal
    delay -- delay (None = auto)
    dimension -- dimensiÃ³n de embedding

    Returns:
    attractor -- puntos del atractor reconstruido
    topological_features -- caracterÃ­sticas topolÃ³gicas
    \"\"\"

    # 1. Crear embedding
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    # 2. Analizar topologÃ­a del atractor
    # (approx. 8 lines)
    # Calcular persistencia y extraer features
    # YOUR CODE STARTS HERE








    # YOUR CODE ENDS HERE

    return attractor, topological_features"""))

# Ejercicio 6
new_cells5.append(nbf.v4.new_markdown_cell("""### Ejercicio 6 - predict_next_event

Predice prÃ³ximo evento usando features topolÃ³gicas.

**AplicaciÃ³n:** PredicciÃ³n de crisis epilÃ©pticas"""))

new_cells5.append(nbf.v4.new_code_cell("""# EJERCICIO 6: PredicciÃ³n de Eventos

def predict_next_event(signal, window_size=500, horizon=100):
    \"\"\"
    Predice si habrÃ¡ evento en prÃ³ximo horizonte.

    Arguments:
    signal -- seÃ±al temporal completa
    window_size -- tamaÃ±o de ventana de anÃ¡lisis
    horizon -- horizonte de predicciÃ³n

    Returns:
    predictions -- predicciones binarias (0/1)
    confidence -- nivel de confianza
    \"\"\"

    # 1. Ventanas deslizantes con features TDA
    # (approx. 10 lines)
    # YOUR CODE STARTS HERE










    # YOUR CODE ENDS HERE

    # 2. DetecciÃ³n de anomalÃ­as
    # (approx. 5 lines)
    # YOUR CODE STARTS HERE





    # YOUR CODE ENDS HERE

    return predictions, confidence"""))

header5 = nbf.v4.new_markdown_cell("""## Ejercicios Avanzados: ReconstrucciÃ³n y PredicciÃ³n

---""")
nb5.cells.insert(insert_idx5, header5)
for i, cell in enumerate(new_cells5):
    nb5.cells.insert(insert_idx5 + 1 + i, cell)

with open('notebooks/05_Series_Temporales_EEG_v2.ipynb', 'w') as f:
    nbf.write(nb5, f)

print("âœ… Tutorial 5 expandido: 6 ejercicios totales (agregados 3 nuevos)")

# =========================================================================
# TUTORIAL 6: Caso Estudio - Agregar 2 ejercicios
# =========================================================================
print("\nğŸ”§ Expandiendo Tutorial 6...")

with open('notebooks/06_Caso_Estudio_Epilepsia_v2.ipynb', 'r') as f:
    nb6 = nbf.read(f, as_version=4)

insert_idx6 = len(nb6.cells) - 1
new_cells6 = []

# Ejercicio 4
new_cells6.append(nbf.v4.new_markdown_cell("""### Ejercicio 4 - feature_importance_analysis

Analiza importancia de caracterÃ­sticas para clasificaciÃ³n.

**Objetivo:** Entender quÃ© features TDA son mÃ¡s discriminativas"""))

new_cells6.append(nbf.v4.new_code_cell("""# EJERCICIO 4: AnÃ¡lisis de Importancia

def feature_importance_analysis(clf, feature_names):
    \"\"\"
    Analiza y visualiza importancia de caracterÃ­sticas.

    Arguments:
    clf -- clasificador entrenado (Random Forest)
    feature_names -- nombres de las caracterÃ­sticas

    Returns:
    importance_dict -- dict {feature: importance}
    top_features -- top 5 features mÃ¡s importantes
    \"\"\"

    # 1. Extraer importancias del modelo
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    # 2. Visualizar
    # (approx. 8 lines)
    # GrÃ¡fico de barras ordenado
    # YOUR CODE STARTS HERE








    # YOUR CODE ENDS HERE

    return importance_dict, top_features"""))

# Ejercicio 5
new_cells6.append(nbf.v4.new_markdown_cell("""### Ejercicio 5 - cross_validate_pipeline

Valida el pipeline completo con cross-validation.

**Objetivo:** Asegurar robustez del modelo"""))

new_cells6.append(nbf.v4.new_code_cell("""# EJERCICIO 5: Cross-Validation Completa

def cross_validate_pipeline(X, y, n_folds=5):
    \"\"\"
    Valida pipeline con k-fold cross-validation.

    Arguments:
    X -- caracterÃ­sticas
    y -- etiquetas
    n_folds -- nÃºmero de folds

    Returns:
    cv_scores -- scores de cada fold
    mean_score -- score promedio
    std_score -- desviaciÃ³n estÃ¡ndar
    \"\"\"
    from sklearn.model_selection import cross_val_score
    from sklearn.ensemble import RandomForestClassifier

    # 1. Setup de cross-validation
    # (approx. 3 lines)
    # YOUR CODE STARTS HERE



    # YOUR CODE ENDS HERE

    # 2. Ejecutar CV y calcular estadÃ­sticas
    # (approx. 5 lines)
    # YOUR CODE STARTS HERE





    # YOUR CODE ENDS HERE

    return cv_scores, mean_score, std_score"""))

header6 = nbf.v4.new_markdown_cell("""## Ejercicios Avanzados: ValidaciÃ³n y AnÃ¡lisis

---""")
nb6.cells.insert(insert_idx6, header6)
for i, cell in enumerate(new_cells6):
    nb6.cells.insert(insert_idx6 + 1 + i, cell)

with open('notebooks/06_Caso_Estudio_Epilepsia_v2.ipynb', 'w') as f:
    nbf.write(nb6, f)

print("âœ… Tutorial 6 expandido: 5 ejercicios totales (agregados 2 nuevos)")

print("\n" + "="*60)
print("ğŸ‰ EXPANSIÃ“N COMPLETADA")
print("="*60)
print("\nResumen:")
print("â€¢ Tutorial 1: 7 ejercicios (4 â†’ 7)")
print("â€¢ Tutorial 2: 7 ejercicios (4 â†’ 7)")
print("â€¢ Tutorial 3: 6 ejercicios (3 â†’ 6)")
print("â€¢ Tutorial 4: 5 ejercicios (3 â†’ 5)")
print("â€¢ Tutorial 5: 6 ejercicios (3 â†’ 6)")
print("â€¢ Tutorial 6: 5 ejercicios (3 â†’ 5)")
print("\nğŸ“Š TOTAL: 36 ejercicios (antes: 20)")
print("    Incremento: +16 ejercicios (+80%)")

EOF

chmod +x expand_all_tutorials.sh
bash expand_all_tutorials.sh

